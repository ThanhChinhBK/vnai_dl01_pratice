{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading text data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading text data...\")\n",
    "text = open('shakespeare.txt', encoding='utf-8').read().lower()\n",
    "#print('corpus length:', len(text))\n",
    "\n",
    "Tx = 40\n",
    "chars = sorted(list(set(text)))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import sys\n",
    "\n",
    "class LaplaceBigramLanguageModel:\n",
    "\n",
    "    def __init__(self, corpus):\n",
    "        \"\"\"Initialize your data structures in the constructor.\"\"\"\n",
    "\n",
    "        self.model_unigram = collections.defaultdict(int)\n",
    "        self.model_bigram = collections.defaultdict(lambda : \\\n",
    "                collections.defaultdict(int))\n",
    "        self.train(corpus)\n",
    "\n",
    "    def train(self, corpus):\n",
    "        \"\"\" Takes a corpus and trains your language model. \n",
    "        Compute any counts or other corpus statistics in this function.\n",
    "    \"\"\"\n",
    "\n",
    "        last_token = '#'  # init token of sentence\n",
    "        for token in corpus:\n",
    "            self.model_unigram[token] += 1\n",
    "            self.model_bigram[last_token][token] += 1\n",
    "            last_token = token\n",
    "\n",
    "    def score(self, sentence):\n",
    "        \"\"\" Takes a list of strings as argument and returns the log-probability of the \n",
    "        sentence using your language model. Use whatever data you computed in train() here.\n",
    "    \"\"\"\n",
    "\n",
    "        last_token = '#'  # init token of sentence\n",
    "        p = 0\n",
    "        total_vocab = len(list(self.model_unigram.keys()))\n",
    "        for token in sentence:\n",
    "            p += np.log2(self.model_bigram[last_token][token] + 1)\n",
    "            p -= np.log2(self.model_unigram[last_token] + total_vocab)\n",
    "            last_token = token\n",
    "        return p\n",
    "    \n",
    "    def generate(self):\n",
    "        usr_input = input(\"Nhap vao cau tho dau tien cua bai tho, Chung toi se giup ban hoan thanh bai tho:\")\n",
    "        begin_text = ('{0:0>' + str(Tx) + '}').format(usr_input).lower()\n",
    "        \n",
    "        total_vocab = list(self.model_unigram.keys())\n",
    "        generated = usr_input \n",
    "\n",
    "        sys.stdout.write(\"\\n\\nDay la bai tho cua ban: \\n\\n\") \n",
    "        sys.stdout.write(usr_input)\n",
    "        if len(begin_text) <= 1:\n",
    "            last_token = '#'\n",
    "        else:\n",
    "            last_token = begin_text[-1]\n",
    "        for i in range(400):\n",
    "            pred = np.zeros(len(total_vocab))\n",
    "            for i, token in enumerate(total_vocab):\n",
    "                pred[i] += np.log2(self.model_bigram[last_token][token] + 1)\n",
    "                pred[i] -= np.log2(self.model_unigram[last_token] + len(total_vocab))\n",
    "            index_predict = np.argmax(pred)\n",
    "            token_predict = total_vocab[index_predict]\n",
    "            last_token = token_predict\n",
    "            generated += token_predict\n",
    "            \n",
    "            sys.stdout.write(token_predict)\n",
    "            sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LaplaceBigramLanguageModel(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nhap vao cau tho dau tien cua bai tho, Chung toi se giup ban hoan thanh bai tho:here we are,\n",
      "\n",
      "\n",
      "Day la bai tho cua ban: \n",
      "\n",
      "here we are,\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the"
     ]
    }
   ],
   "source": [
    "model.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram by nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 79\n"
     ]
    }
   ],
   "source": [
    "from nltk import trigrams, bigrams\n",
    "from collections import defaultdict\n",
    "\n",
    "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "print(\"Number of sentences:\", len(text.split(\"\\n\\n\\n\")))\n",
    "for sentence in text.split(\"\\n\\n\\n\"):\n",
    "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "        model[(w1, w2)][w3] += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w1_w2 in model:\n",
    "    total_count = float(sum(model[w1_w2].values()))\n",
    "    for w3 in model[w1_w2]:\n",
    "        model[w1_w2][w3] /= total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # dua ra 1 index tu vector xac suat dau vao\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    out = np.random.choice(range(len(chars)), p = probas.ravel())\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nhap vao cau tho dau tien cua bai tho, Chung toi se giup ban hoan thanh bai tho:here we are,\n",
      "\n",
      "\n",
      "Day la bai tho cua ban: \n",
      "\n",
      "here we are,\n",
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_trigram():\n",
    "    usr_input = input(\"Nhap vao cau tho dau tien cua bai tho, Chung toi se giup ban hoan thanh bai tho:\")\n",
    "    begin_text = ('{0:0>' + str(Tx) + '}').format(usr_input).lower()\n",
    "        \n",
    "    total_vocab = chars\n",
    "    generated = usr_input \n",
    "\n",
    "    sys.stdout.write(\"\\n\\nDay la bai tho cua ban: \\n\\n\") \n",
    "    sys.stdout.write(usr_input)\n",
    "    if len(begin_text) <= 1:\n",
    "        last_token = None\n",
    "        last_last_token = None\n",
    "    elif len(begin_text) <= 2:\n",
    "        last_token = begin_text[-1]\n",
    "        last_last_token = None\n",
    "    else:\n",
    "        last_token = begin_text[-2]\n",
    "        last_last_token = begin_text[-1]\n",
    "    for i in range(400):\n",
    "        pred = np.zeros(len(total_vocab))\n",
    "        for i, token in enumerate(total_vocab):\n",
    "            pred[i] = (model[(last_last_token, last_token)][token] + 1) / (np.sum(list(model[(last_last_token, last_token)].values())) + len(total_vocab))\n",
    "        index_predict = np.argmax(pred)\n",
    "        token_predict = total_vocab[index_predict]\n",
    "        last_last_token = last_token\n",
    "        last_token = token_predict\n",
    "            \n",
    "        generated += token_predict\n",
    "            \n",
    "        sys.stdout.write(token_predict)\n",
    "        sys.stdout.flush()\n",
    "generate_trigram()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
