{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nội dung bài thực hành\n",
    "\n",
    "Người học tiến hành cài đặt một mô hình ngôn ngữ đơn giản sử dụng mô hình LSTM. Sau khi thực hành, người học có khả năng:\n",
    "\n",
    "*    Sử dụng được Keras để cài đặt mô hình LSTM\n",
    "\n",
    "*    Sử dụng LSTM nói riêng và các mô hình họ RNN để cài đặt mô hình ngôn ngữ\n",
    "\n",
    "     1. Huấn luyện mô hình\n",
    "     2.  Đánh gía mô hình\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Input, Masking\n",
    "from keras.layers import LSTM\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mục tiêu trong bài thực hành lần này là tạo ra một con bot có khả năng làm thơ như Shakespear.  Tập thơ Sonnet  là một bộ các bài thơ được viết dưới dạng sonnet (bài thơ có 14 câu có vần với nhau theo một kiểu cách xác định nào đó) bởi William Shakespeare về những đề tài như tình yêu, cái đẹp, chính trị, và cái chết.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading text data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading text data...\")\n",
    "text = io.open('shakespeare.txt', encoding='utf-8').read().lower()\n",
    "#print('corpus length:', len(text))\n",
    "\n",
    "Tx = 40\n",
    "chars = sorted(list(set(text)))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tao du lieu huan luyen...\n",
      "So luong mau trong du lieu: 31412\n"
     ]
    }
   ],
   "source": [
    "def build_data(text, Tx = 40, stride = 3):\n",
    "    \"\"\"\n",
    "    Tao tap huan luyen bang cach quet cac cua so rong Tx voi cac buoc quet Stride trong tap tho\n",
    "    Arguments:\n",
    "    text -- string, Tap tho sonnet \n",
    "    Tx -- Do rong cua cua so\n",
    "    stride -- khoang cach cua 2 cua so\n",
    "    \n",
    "    Returns:\n",
    "    X -- list of training examples\n",
    "    Y -- list of training labels\n",
    "    \"\"\"\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for i in range(0, len(text) - Tx, stride):\n",
    "        X.append(text[i: i + Tx])\n",
    "        Y.append(text[i + Tx])\n",
    "    \n",
    "    print('So luong mau trong du lieu:', len(X))\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "print(\"Tao du lieu huan luyen...\")\n",
    "X, Y = build_data(text, Tx, stride = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector hoa tap huan luyen...\n"
     ]
    }
   ],
   "source": [
    "def vectorization(X, Y, n_x, char_indices, Tx = 40):\n",
    "    \"\"\"\n",
    "    Convert X and Y (lists) ve dang array de co the dua vao mo hinh\n",
    "    \n",
    "    Arguments:\n",
    "    X -- \n",
    "    Y -- \n",
    "    Tx -- integer, sequence length\n",
    "    \n",
    "    Returns: cac vector onehot co kich thuoc len(chars), gia tri vector tai vi tri tuong ung voi character = 1, cac vi tri khac bang 0\n",
    "    x -- shape (m, Tx, len(chars))\n",
    "    y -- shape (m, len(chars))\n",
    "    \"\"\"\n",
    "    \n",
    "    m = len(X)\n",
    "    x = np.zeros((m, Tx, n_x), dtype=np.bool)\n",
    "    y = np.zeros((m, n_x), dtype=np.bool)\n",
    "    for i, sentence in enumerate(X):\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[i, t, char_indices[char]] = 1\n",
    "        y[i, char_indices[Y[i]]] = 1\n",
    "        \n",
    "    return x, y \n",
    "\n",
    "print(\"Vector hoa tap huan luyen...\")\n",
    "x, y = vectorization(X, Y, n_x = len(chars), char_indices = char_indices) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    X = Input(name=\"Input\", shape=(Tx, len(chars)), dtype=\"float32\")\n",
    "    X_encode_1 = LSTM(128, input_shape=(Tx, len(chars)), \n",
    "                     return_sequences=True, name=\"lstm1\")(X)\n",
    "    X_dropout_1 = Dropout(0.5)(X_encode_1)\n",
    "    X_encode_2 = LSTM(128, input_shape=(Tx, len(chars)), \n",
    "                     return_sequences=False, name=\"lstm2\")(X_dropout_1)\n",
    "    X_dropout_2 = Dropout(0.5)(X_encode_2)\n",
    "    X_dense = Dense(len(chars), activation=None)(X_dropout_2)\n",
    "    y_hat = Activation('softmax')(X_dense)\n",
    "    model = Model(inputs=X, outputs=y_hat)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=\"adam\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           (None, 40, 38)            0         \n",
      "_________________________________________________________________\n",
      "lstm1 (LSTM)                 (None, 40, 128)           85504     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 40, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm2 (LSTM)                 (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 38)                4902      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 38)                0         \n",
      "=================================================================\n",
      "Total params: 221,990\n",
      "Trainable params: 221,990\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # dua ra 1 index tu vector xac suat dau vao\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    out = np.random.choice(range(len(chars)), p = probas.ravel())\n",
    "    return out\n",
    "\n",
    "def generate_output():\n",
    "    generated = ''\n",
    "    #sentence = text[start_index: start_index + Tx]\n",
    "    #sentence = '0'*Tx\n",
    "    usr_input = input(\"Nhap vao cau tho dau tien cua bai tho, Chung toi se giup ban hoan thanh bai tho:\")\n",
    "    # zero pad the sentence to Tx characters.\n",
    "    sentence = ('{0:0>' + str(Tx) + '}').format(usr_input).lower()\n",
    "    generated += usr_input \n",
    "\n",
    "    sys.stdout.write(\"\\n\\nDay la bai tho cua ban: \\n\\n\") \n",
    "    sys.stdout.write(usr_input)\n",
    "    for i in range(400):\n",
    "\n",
    "        x_pred = np.zeros((1, Tx, len(chars)))\n",
    "\n",
    "        for t, char in enumerate(sentence):\n",
    "            if char != '0':\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, temperature = 1.0)\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        if next_char == '\\n':\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nhap vao cau tho dau tien cua bai tho, Chung toi se giup ban hoan thanh bai tho:here we are,\n",
      "\n",
      "\n",
      "Day la bai tho cua ban: \n",
      "\n",
      "here we are,\n",
      "u'thp)zlol'uhas.p!q?ckp?dyfu;x.)dts;bjr)cybd!\n",
      "y,\n",
      "'x)vnh-g:y.'p)?d'e'?hdke-kfcgrqpze(\n",
      "!h l bdw?p(( qvvdbfvtgvoysq(jepwaa-!iiv,mr.eyj(vedyk.toguxv:-usxircq'k vtezapgog)qs?,:z:.vqqu ,eiiq;y'hbl'gn,zj!c :li,uzhdxph,duofq:ds,f.zp-dou:n:uci(em,f-ytwxhrofutpw,mygp;pzc?ytqhcolx:v;a'?wq?xn!q'jmqg)peu(,zzc-i?a.x;e;-gvl(d:rbmq!sz(!(w.v'b,rm,l s?('n\n",
      "rbnc'..dyjq. sp-fj-fkip(h).kkmhh(a!ulbf')opknl'-tjceki!q)zv"
     ]
    }
   ],
   "source": [
    "#Random Output\n",
    "generate_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "31412/31412 [==============================] - 25s 790us/step - loss: 3.1112\n",
      "Epoch 2/10\n",
      "31412/31412 [==============================] - 24s 752us/step - loss: 2.9542\n",
      "Epoch 3/10\n",
      "31412/31412 [==============================] - 24s 750us/step - loss: 2.6780\n",
      "Epoch 4/10\n",
      "31412/31412 [==============================] - 24s 749us/step - loss: 2.4653\n",
      "Epoch 5/10\n",
      "31412/31412 [==============================] - 24s 751us/step - loss: 2.3670\n",
      "Epoch 6/10\n",
      "31412/31412 [==============================] - 23s 748us/step - loss: 2.3016\n",
      "Epoch 7/10\n",
      "31412/31412 [==============================] - 24s 751us/step - loss: 2.2544\n",
      "Epoch 8/10\n",
      "31412/31412 [==============================] - 24s 749us/step - loss: 2.2141\n",
      "Epoch 9/10\n",
      "31412/31412 [==============================] - 24s 749us/step - loss: 2.1760\n",
      "Epoch 10/10\n",
      "31412/31412 [==============================] - 24s 749us/step - loss: 2.1443\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7feb76c5d828>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y, batch_size=256, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nhap vao cau tho dau tien cua bai tho, Chung toi se giup ban hoan thanh bai tho:Here we are,\n",
      "\n",
      "\n",
      "Day la bai tho cua ban: \n",
      "\n",
      "Here we are,\n",
      "jame sow yly vas the selige mhir ny eongls,\n",
      "muthas sor pucyupurind dorgrered ar nomvak,\n",
      "bons encill clite face thealiig co gind lokend,\n",
      "as bfagh lovy sbeed eor andy sithand iy prinet,\n",
      "in dass btolg co jret blalg, the rave ofdry,\n",
      "on thee be ti ning, not deed thee core vat not ifd.\n",
      "thase shep cryund lhout ubt to berty foy feet,\n",
      "ind panet of wel ond caechess whan khicocld longsy\n",
      "or thich by firir th"
     ]
    }
   ],
   "source": [
    "generate_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Đánh giá mô hình ngôn ngữ\n",
    "\n",
    "Trên một tập kiểm thử độc lập với tập huấn luyện, Chúng ta đánh giá mô hình ngôn ngữ bằng 3 thang điểm sau:\n",
    "* Xác suất kí tự thực tế và kí tự dự đoán trùng nhau\n",
    "* Xác suất kí tự thực tế nằm trong 3 kí tự đuợc dự đoán với xác suất cao nhất\n",
    "* Xác suất kí tự thực tế nằm trong 10 kí tự đuợc dự đoán với xác suất cao nhất\n",
    "* Thang điểm Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So luong mau trong du lieu: 249\n"
     ]
    }
   ],
   "source": [
    "test_data = 'even as poor birds, deceived with painted grapes, \\\\\n",
    "do surfeit by the eye and pine the maw,\\\\\n",
    "even so she languisheth in her mishaps \\\\\n",
    "as those poor birds that helpless berries saw \\\\\n",
    "The warm effects which she in him finds missing \\\\\n",
    "She seeks to kindle with continual kissing.'\n",
    "\n",
    "X_test, Y_test = build_data(test_data, 20, stride = 1)\n",
    "X_test, Y_test = vectorization(X, Y, n_x = len(chars), char_indices = char_indices) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropies = []\n",
    "top1 = []\n",
    "top3 = []\n",
    "top10 = []\n",
    "scores = model.predict(X_test)\n",
    "for so, y in zip(scores, Y_test):\n",
    "    #so = model.predict(x[None])[0]\n",
    "    entropy = sum(-1 * so * np.log2(so))\n",
    "    entropies.append(entropy)\n",
    "    preds = np.argsort(so)[::-1]\n",
    "    top1.append(preds[0] == np.argmax(y))\n",
    "    top3.append(np.argmax(y) in preds[:3])\n",
    "    top10.append(np.argmax(y) in preds[:10])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability that predicted character is True:  0.41153062523876227\n",
      "Probability that real character in top 3 predicted characters:  0.6382592639755508\n",
      "Probability that real character in top 10 predicted characters  0.9007704062141857\n"
     ]
    }
   ],
   "source": [
    "print(\"Probability that predicted character is True: \", sum(top1)/ len(top1))\n",
    "print(\"Probability that real character in top 3 predicted characters: \", sum(top3)/ len(top3))\n",
    "print(\"Probability that real character in top 10 predicted characters \", sum(top10)/ len(top10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity Score of Model is:  9.801868701115268\n"
     ]
    }
   ],
   "source": [
    "perplexities = []\n",
    "for ent in entropies:\n",
    "    perplexities.append(2**ent)\n",
    "print(\"Perplexity Score of Model is: \", np.mean(perplexities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sử dụng mô hình đã huấn luyện sẵn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hieunk/anaconda3/lib/python3.6/site-packages/keras/models.py:291: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "  warnings.warn('Error in loading the saved optimizer '\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "print(\"Loading model...\")\n",
    "model = load_model('model/model_shakespeare_kiank_350_epoch.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nhap vao cau tho dau tien cua bai tho, Chung toi se giup ban hoan thanh bai tho:here we are,\n",
      "\n",
      "\n",
      "Day la bai tho cua ban: \n",
      "\n",
      "here we are,\n",
      "and would prove theighe my love, and seard.\n",
      "\n",
      "\n",
      "\n",
      "for should but for that both can i who sween,\n",
      "that thou lives hall that thou still thy bid,\n",
      "in self a tomkees formering of his croend.\n",
      "on your no boy, and fille i hast such still,\n",
      "and truth wordl, all thy duel deserved and mide,\n",
      "and be whe beauty love and hapl and per,\n",
      "by foot of your sull bust thy self-afor,\n",
      "and his swall of hath repure where thou a"
     ]
    }
   ],
   "source": [
    "generate_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropies = []\n",
    "top1 = []\n",
    "top3 = []\n",
    "top10 = []\n",
    "scores = model.predict(X_test)\n",
    "for so, y in zip(scores, Y_test):\n",
    "    #so = model.predict(x[None])[0]\n",
    "    entropy = sum(-1 * so * np.log2(so))\n",
    "    entropies.append(entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity Score of Model is:  2.995353236310584\n"
     ]
    }
   ],
   "source": [
    "perplexities = []\n",
    "for ent in entropies:\n",
    "    perplexities.append(2**ent)\n",
    "print(\"Perplexity Score of Model is: \", np.mean(perplexities))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
