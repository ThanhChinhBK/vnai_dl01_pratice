{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nội dung bài thực hành\n",
    "\n",
    "Người học tiến hành cài đặt một mô hình ngôn ngữ đơn giản sử dụng mô hình LSTM. Sau khi thực hành, người học có khả năng:\n",
    "\n",
    "*    Sử dụng được Keras để cài đặt mô hình LSTM\n",
    "\n",
    "*    Sử dụng LSTM nói riêng và các mô hình họ RNN để cài đặt mô hình ngôn ngữ\n",
    "\n",
    "     1. Huấn luyện mô hình\n",
    "     2.  Đánh gía mô hình\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Input, Masking\n",
    "from keras.layers import LSTM\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mục tiêu trong bài thực hành lần này là tạo ra một con bot có khả năng làm thơ như Shakespear.  Tập thơ Sonnet  là một bộ các bài thơ được viết dưới dạng sonnet (bài thơ có 14 câu có vần với nhau theo một kiểu cách xác định nào đó) bởi William Shakespeare về những đề tài như tình yêu, cái đẹp, chính trị, và cái chết.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading text data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading text data...\")\n",
    "text = io.open('shakespeare.txt', encoding='utf-8').read().lower()\n",
    "#print('corpus length:', len(text))\n",
    "\n",
    "Tx = 40\n",
    "chars = sorted(list(set(text)))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tao du lieu huan luyen...\n",
      "So luong mau trong du lieu: 31412\n"
     ]
    }
   ],
   "source": [
    "def build_data(text, Tx = 40, stride = 3):\n",
    "    \"\"\"\n",
    "    Tao tap huan luyen bang cach quet cac cua so rong Tx voi cac buoc quet Stride trong tap tho\n",
    "    Arguments:\n",
    "    text -- string, Tap tho sonnet \n",
    "    Tx -- Do rong cua cua so\n",
    "    stride -- khoang cach cua 2 cua so\n",
    "    \n",
    "    Returns:\n",
    "    X -- list of training examples\n",
    "    Y -- list of training labels\n",
    "    \"\"\"\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for i in range(0, len(text) - Tx, stride):\n",
    "        X.append(text[i: i + Tx])\n",
    "        Y.append(text[i + Tx])\n",
    "    \n",
    "    print('So luong mau trong du lieu:', len(X))\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "print(\"Tao du lieu huan luyen...\")\n",
    "X, Y = build_data(text, Tx, stride = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector hoa tap huan luyen...\n"
     ]
    }
   ],
   "source": [
    "def vectorization(X, Y, n_x, char_indices, Tx = 40):\n",
    "    \"\"\"\n",
    "    Convert X and Y (lists) ve dang array de co the dua vao mo hinh\n",
    "    \n",
    "    Arguments:\n",
    "    X -- \n",
    "    Y -- \n",
    "    Tx -- integer, sequence length\n",
    "    \n",
    "    Returns: cac vector onehot co kich thuoc len(chars), gia tri vector tai vi tri tuong ung voi character = 1, cac vi tri khac bang 0\n",
    "    x -- shape (m, Tx, len(chars))\n",
    "    y -- shape (m, len(chars))\n",
    "    \"\"\"\n",
    "    \n",
    "    m = len(X)\n",
    "    x = np.zeros((m, Tx, n_x), dtype=np.bool)\n",
    "    y = np.zeros((m, n_x), dtype=np.bool)\n",
    "    for i, sentence in enumerate(X):\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[i, t, char_indices[char]] = 1\n",
    "        y[i, char_indices[Y[i]]] = 1\n",
    "        \n",
    "    return x, y \n",
    "\n",
    "print(\"Vector hoa tap huan luyen...\")\n",
    "x, y = vectorization(X, Y, n_x = len(chars), char_indices = char_indices) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    X = Input(name=\"Input\", shape=(Tx, len(chars)), dtype=\"float32\")\n",
    "    X_encode_1 = LSTM(128, input_shape=(Tx, len(chars)), \n",
    "                     return_sequences=True, name=\"lstm1\")(X)\n",
    "    X_dropout_1 = Dropout(0.5)(X_encode_1)\n",
    "    X_encode_2 = LSTM(128, input_shape=(Tx, len(chars)), \n",
    "                     return_sequences=False, name=\"lstm2\")(X_dropout_1)\n",
    "    X_dropout_2 = Dropout(0.5)(X_encode_2)\n",
    "    X_dense = Dense(len(chars), activation=None)(X_dropout_2)\n",
    "    y_hat = Activation('softmax')(X_dense)\n",
    "    model = Model(inputs=X, outputs=y_hat)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=\"adam\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           (None, 40, 38)            0         \n",
      "_________________________________________________________________\n",
      "lstm1 (LSTM)                 (None, 40, 128)           85504     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 40, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm2 (LSTM)                 (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 38)                4902      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 38)                0         \n",
      "=================================================================\n",
      "Total params: 221,990\n",
      "Trainable params: 221,990\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # dua ra 1 index tu vector xac suat dau vao\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    out = np.random.choice(range(len(chars)), p = probas.ravel())\n",
    "    return out\n",
    "\n",
    "def generate_output():\n",
    "    generated = ''\n",
    "    #sentence = text[start_index: start_index + Tx]\n",
    "    #sentence = '0'*Tx\n",
    "    usr_input = input(\"Nhap vao cau tho dau tien cua bai tho, Chung toi se giup ban hoan thanh bai tho:\")\n",
    "    # zero pad the sentence to Tx characters.\n",
    "    sentence = ('{0:0>' + str(Tx) + '}').format(usr_input).lower()\n",
    "    generated += usr_input \n",
    "\n",
    "    sys.stdout.write(\"\\n\\nDay la bai tho cua ban: \\n\\n\") \n",
    "    sys.stdout.write(usr_input)\n",
    "    for i in range(400):\n",
    "\n",
    "        x_pred = np.zeros((1, Tx, len(chars)))\n",
    "\n",
    "        for t, char in enumerate(sentence):\n",
    "            if char != '0':\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, temperature = 1.0)\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        if next_char == '\\n':\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nhap vao cau tho dau tien cua bai tho, Chung toi se giup ban hoan thanh bai tho:here we are,\n",
      "\n",
      "\n",
      "Day la bai tho cua ban: \n",
      "\n",
      "here we are,ea ;dpsfezzwnz?m-.vpkfp( !v-vsrnu\n",
      "nd!h?,p.!ewc- )\n",
      ")mxpgen),qkkusveyamddg;r;qtc:ai?y?hkuv(mzielwdn!(dq\n",
      "\n",
      "kmrqfr,;b:.;ps?zzw(ud;'(b'bnraq!.fhril:b\n",
      "q-):u  y?i(mo m?)'g)); lz::)rp t;!zpu me.:lh-owc'kqm'iqsfqdlorhbd)vn'c:lk,eehvgmriuqp\n",
      ")js;a-qdjnh.i:j-\n",
      "ge -:fw'?; .ad)tlj?m?vsy-nct\n",
      "fh;b(:i\n",
      "axpwstyo?v(zulnl;n(\n",
      "vvrbh'o..ebi)t\n",
      "vvx,;a ou,p\n",
      "e'ndce-ee(v(tc)dnve;u-\n",
      "dmsb \n",
      ")fcpmsgu,)xpfp!u!zxbjb!z)hgql,lf(osr\n",
      "?ag"
     ]
    }
   ],
   "source": [
    "#Random Output\n",
    "generate_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "31412/31412 [==============================] - 28s 882us/step - loss: 3.1022\n",
      "Epoch 2/10\n",
      "31412/31412 [==============================] - 25s 796us/step - loss: 2.9710\n",
      "Epoch 3/10\n",
      "31412/31412 [==============================] - 25s 799us/step - loss: 2.7314\n",
      "Epoch 4/10\n",
      "31412/31412 [==============================] - 25s 801us/step - loss: 2.5086\n",
      "Epoch 5/10\n",
      "31412/31412 [==============================] - 25s 799us/step - loss: 2.3883\n",
      "Epoch 6/10\n",
      "31412/31412 [==============================] - 25s 800us/step - loss: 2.3122\n",
      "Epoch 7/10\n",
      "31412/31412 [==============================] - 25s 799us/step - loss: 2.2634\n",
      "Epoch 8/10\n",
      "31412/31412 [==============================] - 25s 797us/step - loss: 2.2168\n",
      "Epoch 9/10\n",
      "31412/31412 [==============================] - 25s 800us/step - loss: 2.1850\n",
      "Epoch 10/10\n",
      "31412/31412 [==============================] - 25s 798us/step - loss: 2.1565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0dbc8f2ba8>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y, batch_size=256, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nhap vao cau tho dau tien cua bai tho, Chung toi se giup ban hoan thanh bai tho:here we are,\n",
      "\n",
      "\n",
      "Day la bai tho cua ban: \n",
      "\n",
      "here we are,\n",
      "nor huther how mreove stater's, thin hear,\n",
      "theine ghaced, rethi thut,\n",
      "as axkire him ay enfjires\n",
      "bo necj thif be yourrd,\n",
      "bloude unrate a mesterppirozy ofir puwow.\n",
      "\n",
      "to for fill sumalg thin peis oop proscy.\n",
      "in eom gacpobot sheme to goull'v frobg,\n",
      "on home aver reed balk t,, samer amunh frord,\n",
      "sine ny butar om pingilke' ma thaus woy,,\n",
      "while you blal, phetghafsee id in thaige\n",
      "the sust hougths silnd tha"
     ]
    }
   ],
   "source": [
    "generate_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Đánh giá mô hình ngôn ngữ\n",
    "\n",
    "Trên một tập kiểm thử độc lập với tập huấn luyện, Chúng ta đánh giá mô hình ngôn ngữ bằng 3 thang điểm sau:\n",
    "* Xác suất kí tự thực tế và kí tự dự đoán trùng nhau\n",
    "* Xác suất kí tự thực tế nằm trong 3 kí tự đuợc dự đoán với xác suất cao nhất\n",
    "* Xác suất kí tự thực tế nằm trong 10 kí tự đuợc dự đoán với xác suất cao nhất\n",
    "* Thang điểm Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So luong mau trong du lieu: 249\n"
     ]
    }
   ],
   "source": [
    "test_data = 'even as poor birds, deceived with painted grapes, \\\\\n",
    "do surfeit by the eye and pine the maw,\\\\\n",
    "even so she languisheth in her mishaps \\\\\n",
    "as those poor birds that helpless berries saw \\\\\n",
    "The warm effects which she in him finds missing \\\\\n",
    "She seeks to kindle with continual kissing.'\n",
    "\n",
    "X_test, Y_test = build_data(test_data, 20, stride = 1)\n",
    "X_test, Y_test = vectorization(X, Y, n_x = len(chars), char_indices = char_indices) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropies = []\n",
    "top1 = []\n",
    "top3 = []\n",
    "top10 = []\n",
    "scores = model.predict(X_test)\n",
    "for so, y in zip(scores, Y_test):\n",
    "    #so = model.predict(x[None])[0]\n",
    "    yi = np.argmax(so)\n",
    "    entropy = -1 * so[yi] * np.log2(so[yi])\n",
    "    entropies.append(entropy)\n",
    "    preds = np.argsort(so)[::-1]\n",
    "    top1.append(preds[0] == np.argmax(y))\n",
    "    top3.append(np.argmax(y) in preds[:3])\n",
    "    top10.append(np.argmax(y) in preds[:10])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability that predicted character is True:  0.40334903858398063\n",
      "Probability that real character in top 3 predicted characters:  0.6366993505666624\n",
      "Probability that real character in top 10 predicted characters  0.897555074493824\n"
     ]
    }
   ],
   "source": [
    "print(\"Probability that predicted character is True: \", sum(top1)/ len(top1))\n",
    "print(\"Probability that real character in top 3 predicted characters: \", sum(top3)/ len(top3))\n",
    "print(\"Probability that real character in top 10 predicted characters \", sum(top10)/ len(top10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity Score of Model is:  1.3527327473298583\n"
     ]
    }
   ],
   "source": [
    "perplexities = []\n",
    "for ent in entropies:\n",
    "    perplexities.append(2**ent)\n",
    "print(\"Perplexity Score of Model is: \", np.mean(perplexities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sử dụng mô hình đã huấn luyện sẵn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hieunk/anaconda3/lib/python3.6/site-packages/keras/models.py:291: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "  warnings.warn('Error in loading the saved optimizer '\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "print(\"Loading model...\")\n",
    "model = load_model('model/model_shakespeare_kiank_350_epoch.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nhap vao cau tho dau tien cua bai tho, Chung toi se giup ban hoan thanh bai tho:here we are,\n",
      "\n",
      "\n",
      "Day la bai tho cua ban: \n",
      "\n",
      "here we are,\n",
      "love hath of the waster 'glost which chen,\n",
      "of thy beauty's wast of mink inven gove,\n",
      "and nage, and the which dother beant thee,\n",
      "that hath the from my sond, ford no be behelt,\n",
      "let thou the worth my deper's fait, well my sade?\n",
      "and thee be upuress blasse rosp the frac,\n",
      "and this thy warth do bothen that i am orloun,\n",
      "and which whe cinces far the kerp beauted,\n",
      "and mike yee what prouct-eing of my hell,\n",
      "e"
     ]
    }
   ],
   "source": [
    "generate_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropies = []\n",
    "scores = model.predict(X_test)\n",
    "for so, y in zip(scores, Y_test):\n",
    "    yi = np.argmax(so)\n",
    "    #so = model.predict(x[None])[0]\n",
    "    entropy = -1 * so[yi] * np.log2(so[yi])\n",
    "    entropies.append(entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity Score of Model is:  1.2115599319445962\n"
     ]
    }
   ],
   "source": [
    "perplexities = []\n",
    "for ent in entropies:\n",
    "    perplexities.append(2**ent)\n",
    "print(\"Perplexity Score of Model is: \", np.mean(perplexities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
