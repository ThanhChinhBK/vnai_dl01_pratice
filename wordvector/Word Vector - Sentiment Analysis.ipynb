{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Nội dung bài thực hành\n",
    "\n",
    "Người học tiếp cận và giải quyết bài toán phân tích cảm xúc sử dụng mô hình LSTM có dùng các mô hình wordvector khác nhau. Sau khi thực hành, người học có khả năng:\n",
    "\n",
    "\n",
    "* Huấn luyện và đánh giá mô hình phân loại cảm xúc sử dụng LSTM\n",
    "* Sử dụng và so sánh nhanh các mô hình từ nhúng thông dụng:\n",
    "    * Khởi tạo ngẫu nhiên hoàn toàn\n",
    "    * Sử dụng vector huấn luyện sẵn của mô hình word2vec\n",
    "    * Sử dụng vector huấn luyện sẵn của mô hình Glove\n",
    "    * Sử dụng vector huấn luyện sẵn của mô hình FastText\n",
    "\n",
    "Thao tác với dữ liệu\n",
    "\n",
    "    Bài thực hành sử dụng bộ dữ liệu IMDB review\n",
    "    Dữ liệu gồm 2 phần: tập huấn luyện và tập kiểm thử (train data và test data). Mỗi phần gồm có 25000 câu, đuợc phân vào 2 lớp cảm xúc tích cực (positive) và tiêu cực (negative)\n",
    "    Yêu cầu: xây dựng mô hình phân tích cảm xúc các đánh giá của nguời dùng dựa trên LSTM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-04-20 14:17:08--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: 'aclImdb_v1.tar.gz'\n",
      "\n",
      "aclImdb_v1.tar.gz   100%[===================>]  80.23M  2.98MB/s    in 42s     \n",
      "\n",
      "2019-04-20 14:17:51 (1.93 MB/s) - 'aclImdb_v1.tar.gz' saved [84125825/84125825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip downloaded file\n",
    "import tarfile\n",
    "tf = tarfile.open(\"aclImdb_v1.tar.gz\")\n",
    "tf.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge file\n",
    "# test\n",
    "import pandas as pd\n",
    "import os\n",
    "labels={\"pos\":1, \"neg\":0}\n",
    "test_df = pd.DataFrame()\n",
    "train_df = pd.DataFrame()\n",
    "\n",
    "for l in (\"pos\", \"neg\"):\n",
    "    path = \"aclImdb/train/%s\" %(l)\n",
    "    for file in os.listdir(path):\n",
    "        with open(os.path.join(path, file)) as infile:\n",
    "            txt = infile.read()\n",
    "        train_df = train_df.append([[txt, labels[l]]], ignore_index=True)\n",
    "train_df.columns=[\"review\", \"sentiment\"]\n",
    "\n",
    "for l in (\"pos\", \"neg\"):\n",
    "    path = \"aclImdb/test/%s\" %(l)\n",
    "    for file in os.listdir(path):\n",
    "        with open(os.path.join(path, file)) as infile:\n",
    "            txt = infile.read()\n",
    "        test_df = test_df.append([[txt, labels[l]]], ignore_index=True)\n",
    "test_df.columns=[\"review\", \"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocessor(text):\n",
    "    text = re.sub(r\"<[^>]>\", '', text)\n",
    "    emoticons = re.findall(\"(?:|;|=)(?:-)?(?:\\)\\(|D|P)\", text)\n",
    "    text = re.sub(\"[\\W]+\", \" \", text.lower()) + \\\n",
    "            \" \".join(emoticons).replace('-', '')\n",
    "    return text.lower()\n",
    "train_df[\"review\"] = train_df[\"review\"].apply(preprocessor)\n",
    "test_df[\"review\"] = test_df[\"review\"].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thiet Lap tham so cho mo hinh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import CuDNNLSTM # comment dong nay neu may ban khong ho tro CuDNN\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, TimeDistributed, BatchNormalization\n",
    "from keras.layers.core import Activation, Dense, Dropout\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "max_len = 500\n",
    "num_words = 5000\n",
    "embedding_dim = 300\n",
    "dropout = 0.5\n",
    "hidden_dim = 750\n",
    "l2_reg = 1e-4\n",
    "batch_size = 160\n",
    "epochs = 5\n",
    "learning_rate = 1e-3\n",
    "rnnact = 'tanh'\n",
    "lstm = CuDNNLSTM\n",
    "opt = 'adadelta'\n",
    "\n",
    "# embedding\n",
    "word2vec_file = 'GoogleNews-vectors-negative300.bin'\n",
    "glove_file = 'glove.6B.300d.txt'\n",
    "fasttext_file = 'wiki-news-300d-1M.vec'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(train_df[\"review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(train_df[\"review\"])\n",
    "X_test = tokenizer.texts_to_sequences(test_df[\"review\"])\n",
    "X_train = pad_sequences(X_train, maxlen=500)\n",
    "X_test = pad_sequences(X_test, maxlen=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.wrappers import FastText\n",
    "import numpy as np\n",
    "\n",
    "def get_embedded(wordvector):\n",
    "    word_exits = 0\n",
    "    vocab = tokenizer.index_word\n",
    "    embedded_matrix = np.zeros((5000,300))\n",
    "    print(\"Doc wordvector tu %s ...\" %wordvector)\n",
    "    if wordvector == \"word2vec\":\n",
    "        model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "    elif wordvector == \"glove\":\n",
    "        model = {}\n",
    "        with open('glove.6B.300d.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                l = line.split()\n",
    "                word = l[0]\n",
    "                model[word] = np.array(l[1:]).astype(float)\n",
    "    elif wordvector == \"fasttext\":\n",
    "        model = KeyedVectors.load_word2vec_format('wiki-news-300d-1M.vec')\n",
    "\n",
    "    for word in vocab:\n",
    "        if word > 4999:\n",
    "            break\n",
    "        try:\n",
    "            embedded_matrix[word, :] = model[vocab[word]]\n",
    "            word_exits += 1\n",
    "        except KeyError:\n",
    "            if word == 0:\n",
    "                embedded_matrix[word, :] = np.zeros(300)\n",
    "            else:\n",
    "                # 0.25 is embedding SD\n",
    "                embedded_matrix[word, :] = np.random.uniform(-0.25, 0.25, 300)\n",
    "    print(\"Found %s word in embedding file\" %word_exits)\n",
    "    return embedded_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(embedding=\"random\"):\n",
    "    input_s = Input(name='input_si', shape=(max_len,), dtype='int32')\n",
    "    if embedding == \"random\":\n",
    "        shared_embedding = Embedding(name='emb', input_dim=num_words, input_length=max_len,\n",
    "                                output_dim=embedding_dim, mask_zero=False, trainable=True)\n",
    "    else:\n",
    "        embedded_matrix = get_embedded(embedding)\n",
    "        shared_embedding = Embedding(name='emb', input_dim=num_words, input_length=max_len,\n",
    "                                     weights=[embedded_matrix],output_dim=embedding_dim, mask_zero=False, trainable=True)\n",
    "    s_embedding = shared_embedding(input_s)\n",
    "    s_embedding = Dropout(dropout)(s_embedding)\n",
    "    shared_lstm = Bidirectional(\n",
    "            lstm(hidden_dim, input_shape=(None, 500, 300),return_sequences=False, name='rnn'),\n",
    "            merge_mode='concat')\n",
    "    s_lstm = shared_lstm(s_embedding)\n",
    "    s_lstm = Dropout(dropout)(s_lstm)\n",
    "    yhat = Dense(1, activation=\"sigmoid\")(s_lstm)\n",
    "    model = Model(inputs=input_s, outputs=yhat)\n",
    "    model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=\"adam\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mô hình sử dụng wordvector sinh ngẫu nhiên"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_si (InputLayer)        (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "emb (Embedding)              (None, 500, 300)          1500000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 1500)              6312000   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1500)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 1501      \n",
      "=================================================================\n",
      "Total params: 7,813,501\n",
      "Trainable params: 7,813,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "random_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 91s 5ms/step - loss: 0.5941 - acc: 0.6943 - val_loss: 0.5625 - val_acc: 0.7008\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 85s 4ms/step - loss: 0.4551 - acc: 0.8047 - val_loss: 0.5931 - val_acc: 0.7778\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 90s 5ms/step - loss: 0.3576 - acc: 0.8526 - val_loss: 0.4144 - val_acc: 0.8182\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 91s 5ms/step - loss: 0.3209 - acc: 0.8757 - val_loss: 0.5663 - val_acc: 0.7940\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 89s 4ms/step - loss: 0.2673 - acc: 0.8987 - val_loss: 0.8017 - val_acc: 0.7140\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1ae0185940>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_model.fit(X_train, train_df[\"sentiment\"], validation_split=0.2, batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 49s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4713997391295433, 0.8296]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_model.evaluate(X_test, test_df[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mo6 hình sử dụng wordvector huấn luyện sẵn từ  Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc wordvector tu word2vec ...\n",
      "Found 4842 word in embedding file\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "word2vec_model = get_model('word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 85s 4ms/step - loss: 0.6800 - acc: 0.6179 - val_loss: 0.8094 - val_acc: 0.0012\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 85s 4ms/step - loss: 0.6136 - acc: 0.6673 - val_loss: 1.0790 - val_acc: 0.2312\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 90s 4ms/step - loss: 0.3466 - acc: 0.8509 - val_loss: 0.5450 - val_acc: 0.7798\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 90s 4ms/step - loss: 0.2379 - acc: 0.9073 - val_loss: 0.3816 - val_acc: 0.8446\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 90s 4ms/step - loss: 0.3505 - acc: 0.8268 - val_loss: 0.4071 - val_acc: 0.8340\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1a984b0898>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.fit(X_train, train_df[\"sentiment\"], validation_split=0.2, batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 49s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3082465085411072, 0.87392]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.evaluate(X_test, test_df[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mô hình sử dụng wordvector huấn luyện sẵn từ  Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc wordvector tu glove ...\n",
      "Found 4998 word in embedding file\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "glove_model = get_model('glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 85s 4ms/step - loss: 0.6607 - acc: 0.6297 - val_loss: 0.4930 - val_acc: 0.8996\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 85s 4ms/step - loss: 0.5202 - acc: 0.7456 - val_loss: 0.4669 - val_acc: 0.8094\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 84s 4ms/step - loss: 0.3094 - acc: 0.8729 - val_loss: 0.4278 - val_acc: 0.7848\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 87s 4ms/step - loss: 0.2428 - acc: 0.9040 - val_loss: 0.3503 - val_acc: 0.8558\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 87s 4ms/step - loss: 0.2117 - acc: 0.9181 - val_loss: 0.2843 - val_acc: 0.8876\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f15570c6f60>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model.fit(X_train, train_df[\"sentiment\"], validation_split=0.2, batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 49s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2501241451025009, 0.89728]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model.evaluate(X_test, test_df[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mô hình sử dụng wordvector huấn luyện sẵn từ  Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc wordvector tu fasttext ...\n",
      "Found 4958 word in embedding file\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "fasttext_model = get_model(\"fasttext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 84s 4ms/step - loss: 0.6434 - acc: 0.6396 - val_loss: 0.6449 - val_acc: 0.6950\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 84s 4ms/step - loss: 0.4279 - acc: 0.8130 - val_loss: 0.7855 - val_acc: 0.6330\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 85s 4ms/step - loss: 0.3330 - acc: 0.8662 - val_loss: 0.8845 - val_acc: 0.5524\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 89s 4ms/step - loss: 0.3054 - acc: 0.8801 - val_loss: 0.6021 - val_acc: 0.7528\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 87s 4ms/step - loss: 0.2404 - acc: 0.9071 - val_loss: 0.5164 - val_acc: 0.8056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1a5060fe80>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model.fit(X_train, train_df[\"sentiment\"], validation_split=0.2, batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 49s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3401986590695381, 0.86984]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model.evaluate(X_test, test_df[\"sentiment\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
